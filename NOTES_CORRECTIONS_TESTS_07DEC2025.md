# ğŸ“ NOTES DE CORRECTIONS - Session de Tests 07 DÃ©cembre 2025

## Tests EffectuÃ©s
- Sophie (PM) âœ… - Extrait 20 BRs correctement
- Olivia (BA) âœ… - GÃ©nÃ¨re 5 UCs cohÃ©rents pour BR-001
- Marcus (Architect) âœ… - Solution Design complet et pertinent
- Aisha (Data Migration) âš ï¸ - Output overkill / mal utilisÃ©e

---

## ğŸš¨ PROBLÃˆME #1 : Aisha appelÃ©e Ã  tort

**Constat :**
- Aisha gÃ©nÃ¨re un plan de migration de 115 pages pour une simple capture de leads
- Elle suppose un "Legacy CRM Oracle 12c" avec 250,000 records Ã  migrer
- Budget estimÃ© $198,000 pour 9 personnes sur 12 semaines
- ComplÃ¨tement hors sujet pour un projet greenfield

**Cause :**
- Le prompt d'Aisha est conÃ§u pour des **migrations de donnÃ©es**
- Elle ne devrait Ãªtre appelÃ©e QUE si :
  1. Il y a un systÃ¨me existant avec donnÃ©es Ã  migrer
  2. Marcus identifie explicitement un besoin de migration

**Correction Ã  faire :**
1. Ajouter une logique de dÃ©cision dans l'orchestrateur :
   - SI project_type == "greenfield" ALORS skip Aisha
   - SI project_type == "migration" OU Marcus.output contient "legacy system" ALORS call Aisha
2. Ou demander explicitement dans le brief initial si c'est une migration ou un nouveau projet
3. Modifier le workflow pour rendre Aisha optionnelle selon le contexte

---

## ğŸ”§ PROBLÃˆME #2 : Format d'input incohÃ©rent entre agents

**Constat :**
- Sophie : accepte texte brut â†’ retourne JSON structurÃ©
- Olivia : attend JSON (BR) â†’ retourne JSON (UCs)
- Marcus : attend JSON (UCs + summary) â†’ retourne JSON (architecture)
- Aisha : attend texte brut â†’ retourne markdown + JSON

**Correction Ã  faire :**
1. Standardiser les formats d'input/output pour faciliter le chaÃ®nage
2. Documenter clairement le "contrat" de chaque agent
3. CrÃ©er des transformateurs automatiques entre agents si nÃ©cessaire

---

## ğŸ“‹ PROBLÃˆME #3 : Agents tentent de dÃ©ployer vers Salesforce

**Constat :**
- Sophie et Aisha tentent un "DÃ©ploiement vers Salesforce" Ã  la fin
- Ã‰chec car ils n'ont pas de code Ã  dÃ©ployer

**Correction Ã  faire :**
1. Conditionner le dÃ©ploiement au type d'agent (seulement pour Diego, Zara, Raj)
2. Ou dÃ©sactiver complÃ¨tement le dÃ©ploiement en mode "test"

---

## ğŸ“‹ PROBLÃˆME #4 : Langue de sortie

**Constat :**
- Input en franÃ§ais â†’ Output en anglais
- Pas forcÃ©ment un problÃ¨me pour des livrables techniques Salesforce
- Mais pourrait Ãªtre configurable

**Correction optionnelle :**
- Ajouter paramÃ¨tre de langue dans le projet
- Adapter les prompts en fonction

---

## âœ… POINTS POSITIFS

1. **CohÃ©rence des outputs** : Les UCs d'Olivia correspondent bien au BR, l'architecture de Marcus utilise les objets dÃ©finis par Olivia
2. **RAG fonctionnel** : Le contexte Salesforce est bien intÃ©grÃ© (Web-to-Lead, Assignment Rules, etc.)
3. **QualitÃ© professionnelle** : Les livrables sont dÃ©taillÃ©s et exploitables
4. **ChaÃ®nage manuel rÃ©ussi** : On peut passer les outputs d'un agent Ã  l'autre

---

## ğŸ¯ PROCHAINES CORRECTIONS PRIORITAIRES

1. [ ] Rendre Aisha conditionnelle (migration only)
2. [ ] DÃ©sactiver dÃ©ploiement SF pour agents non-dev
3. [ ] Documenter format input/output de chaque agent
4. [ ] Tester Elena, Jordan, Lucas pour complÃ©ter la chaÃ®ne


---

## 5. ELENA (QA Engineer) - Observations

**ExÃ©cution #95 - Input:** RÃ©sumÃ© texte architecture Marcus

### Points positifs
- Structure professionnelle et dÃ©taillÃ©e
- Environnements complets (5 tiers avec config)
- MÃ©triques prÃ©cises (85% coverage, <500ms API, etc.)
- Format test cases exploitable (template 15 sections)
- WCAG 2.1 AA accessibility inclus
- Risk matrix avec mitigations
- Timeline dÃ©taillÃ©e sur 9 semaines
- RAG fonctionnel (12,778 chars)

### Points Ã  vÃ©rifier/corriger
1. **Specs excessives** - Test Strategy & Methodology trÃ¨s dÃ©taillÃ©es, peut-Ãªtre trop pour certains projets
2. **Document tronquÃ©** - 52,321 chars gÃ©nÃ©rÃ©s, s'arrÃªte Ã  TEST-033 sur 280+ prÃ©vus
   - Cause: Limite tokens Haiku (26,593) atteinte
   - Solution: Utiliser Sonnet pour documents longs OU dÃ©couper en sections
3. **Ã€ vÃ©rifier dans SDS** - Comment ces specs QA se transmettent dans le document final

### ProblÃ¨me technique RAG
```
âŒ Erreur: No module named 'sentence_transformers'
âš ï¸ Reranker non disponible
âœ… Fallback fonctionne (OpenAI embeddings probable)
```
**Fix:** `docker exec digital-humans-backend pip install sentence-transformers`


---

## 6. PROBLÃˆME GLOBAL - sentence_transformers manquant

**SymptÃ´me:** ApparaÃ®t sur tous les agents testÃ©s
```
âš ï¸ Reranker non disponible: No module named 'sentence_transformers'
```

**Impact:** 
- RAG fonctionne quand mÃªme (fallback) mais sans reranking
- Contexte RAG rÃ©cupÃ©rÃ©: 12-15K chars (OK)
- QualitÃ© potentiellement dÃ©gradÃ©e sans reranking des rÃ©sultats

**Fix Ã  appliquer:**
```bash
docker exec digital-humans-backend pip install sentence-transformers
docker restart digital-humans-backend
```

**Statut:** Ã€ corriger aprÃ¨s les tests


---

## 7. JORDAN (DevOps Engineer) - Observations

**ExÃ©cution #97 - Input:** RÃ©sumÃ© composants Ã  dÃ©ployer + environnements

### Points positifs
- Structure professionnelle (Table of contents, 10 sections prÃ©vues)
- CI/CD Pipeline complet avec GitHub Actions YAML (~500 lignes)
- Diagrammes Mermaid pertinents (pipeline flow, environment architecture)
- Environment Strategy dÃ©taillÃ©e (5 tiers avec specs)
- Scripts bash exÃ©cutables (deploy.sh, rollback.sh)
- RAG fonctionnel (14,353 chars)

### ProblÃ¨me identique Ã  Elena : DOCUMENT TRONQUÃ‰

**Statistiques:**
- Output gÃ©nÃ©rÃ©: 54,527 chars
- Tokens utilisÃ©s: 22,395 (limite Haiku atteinte)
- Document annoncÃ©: 115 pages
- Document rÃ©el: ~25 pages (Sections 1-4 partielles)

**Contenu manquant:**
- Fin de Section 4 (Deployment Automation) - script tronquÃ© Ã  log_info()
- Section 5: Monitoring & Alerting (non gÃ©nÃ©rÃ©e)
- Section 6: Backup & Disaster Recovery (non gÃ©nÃ©rÃ©e)
- Section 7: Release Management (non gÃ©nÃ©rÃ©e)
- Section 8: Version Control Strategy (non gÃ©nÃ©rÃ©e)
- Section 9: Security in DevOps (non gÃ©nÃ©rÃ©e)
- Section 10: Performance Optimization (non gÃ©nÃ©rÃ©e)

**Cause:** Limite tokens Claude Haiku 4.5

### Solution proposÃ©e
1. **Option A:** Utiliser Sonnet pour agents gÃ©nÃ©rant documents longs (Elena, Jordan, Marcus)
2. **Option B:** DÃ©couper gÃ©nÃ©ration en sections (plusieurs appels LLM)
3. **Option C:** RÃ©duire verbositÃ© des prompts pour outputs plus concis


---

## 8. POINT Ã€ CREUSER : Limites tokens par tier

**Configuration actuelle:**
- PM (Sophie): Opus â†’ 32K output tokens
- BA (Olivia): Sonnet â†’ 64K output tokens  
- Architect (Marcus): Sonnet â†’ 64K output tokens
- Workers (Diego, Zara, Raj, Elena, Jordan, Aisha, Lucas): Haiku â†’ 8K output tokens

**ProblÃ¨me observÃ©:**
Elena et Jordan (workers) gÃ©nÃ¨rent des specs de 50K+ chars mais Haiku limite Ã  ~25-30K chars.

**Question Ã  creuser:**
- Pour la phase SPEC (gÃ©nÃ©ration documents), les workers auraient-ils besoin de Sonnet ?
- DiffÃ©rencier tier par PHASE ? (Spec = Sonnet, ImplÃ©mentation = Haiku)
- Ou rÃ©duire verbositÃ© des prompts workers pour tenir dans 8K tokens ?

**Impact coÃ»t estimÃ©:**
- Haiku: ~$0.25/1M input, $1.25/1M output
- Sonnet: ~$3/1M input, $15/1M output
- Passer workers en Sonnet = ~12x plus cher pour ces agents


---

## 9. SOPHIE (PM) - AmÃ©lioration nÃ©cessaire des descriptions BR

**Test #99 - Input complexe:** SystÃ¨me de gestion des demandes de service (Service Requests)

### Observation
Sophie gÃ©nÃ¨re 27 BRs bien structurÃ©s et cohÃ©rents (pas d'hallucinations), mais les descriptions sont trop gÃ©nÃ©riques.

**Exemple BR-004:**
- **Titre:** Customer Information Association
- **Description actuelle:** "Each service request must be linked to the customer who submitted it, including contact information."

**Ce qui manque:**
- Quels champs spÃ©cifiques ? (nom, email, tÃ©lÃ©phone, company ?)
- Type de relation Salesforce ? (Lookup vers Contact, Account, ou les deux ?)
- Gestion des clients inconnus ? (crÃ©ation auto ou erreur ?)
- Champs obligatoires vs optionnels ?

### Impact
Olivia (BA) doit "inventer" les dÃ©tails manquants pour gÃ©nÃ©rer les Use Cases, ce qui peut crÃ©er des incohÃ©rences ou des hallucinations en aval.

### Solution proposÃ©e
Modifier le prompt de Sophie pour :
1. Exiger 3-5 phrases minimum par description
2. Demander les champs/donnÃ©es spÃ©cifiques attendus
3. PrÃ©ciser les rÃ¨gles mÃ©tier associÃ©es
4. Identifier les dÃ©pendances avec d'autres BRs

**Format amÃ©liorÃ© suggÃ©rÃ©:**
```
BR-004: Customer Information Association
Description: Each service request must be linked to the customer record.
Fields Required: Contact (Lookup - required), Account (Lookup - auto-populated from Contact)
Business Rules: 
- Contact must exist in system before SR creation
- Account auto-populated via Contact.AccountId
- If no Contact found, prompt user to create one
Dependencies: BR-002 (unique ID), BR-003 (description)
```



---

## 9. ğŸš¨ BUG CRITIQUE RÃ‰SOLU : Mapping BR workflow â†’ Olivia

**Date:** 07 DÃ©cembre 2025 - 17h45
**ExÃ©cutions concernÃ©es:** #88 (bug), #103 (bug), #104 (fix validÃ©)

### SymptÃ´me
Lors de l'exÃ©cution via PM Orchestrator (workflow complet), les UCs gÃ©nÃ©rÃ©es par Olivia Ã©taient **complÃ¨tement hors sujet** :

| Requirement (leads automobiles) | UCs gÃ©nÃ©rÃ©es (exec #88) âŒ |
|--------------------------------|---------------------------|
| "Lead capture from website, phone, email, partner portals" | "Business User Creates Custom Data Model Object" |
| | "Forecast Manager Configures Consumption-Based Forecast" |
| | "Sales Rep Syncs Email Communication to Salesforce" |
| | "Gmail Integration", "Einstein Activity Capture"... |

**Paradoxe :** Les tests individuels avec le mÃªme BR via le testeur produisaient des UCs cohÃ©rentes.

### Cause racine identifiÃ©e

**Fonction `_get_validated_brs()` dans `pm_orchestrator_service_v2.py` :**

```python
# AVANT (bug) - ligne 1185
return [
    {
        "id": br.br_id,
        "category": br.category,
        "requirement": br.requirement,  # âŒ Olivia cherche "description"
        "priority": br.priority.value,  # âŒ Format "should" au lieu de "SHOULD_HAVE"
    }
    for br in brs
]
```

**Olivia construit sa query RAG avec :**
```python
query = f"Salesforce {br.get('category', '')} {br.get('title', '')} {br.get('description', '')}"
```

**RÃ©sultat :** `title` et `description` Ã©taient vides â†’ query RAG = `"Salesforce DATA_MODEL "` â†’ RAG retournait du contenu gÃ©nÃ©rique hors sujet.

### Correction appliquÃ©e

```python
# APRÃˆS (fix) - ligne 1185
return [
    {
        "id": br.br_id,
        "title": br.br_id,  # âœ… AjoutÃ© (BR ID comme fallback)
        "description": br.requirement,  # âœ… MappÃ© correctement
        "category": br.category or "OTHER",
        "priority": (br.priority.value.upper() + "_HAVE") if br.priority else "SHOULD_HAVE",
        "stakeholder": "Business User"  # âœ… AjoutÃ©
    }
    for br in brs
]
```

### Validation du fix

**Exec #104 (aprÃ¨s fix) :**
```
UC-001-01: Capture Lead from Website Web-to-Lead Form âœ…
UC-001-02: Manual Lead Entry by Sales Representative via Phone Call âœ…
UC-001-03: Capture Lead from Email Inquiry via Email-to-Lead âœ…
UC-001-04: Bulk Import Leads from Partner Portal via Data Import âœ…
UC-001-05: Capture Lead from LinkedIn Lead Gen Form Integration âœ…
```

**100% cohÃ©rent** avec le BR "lead capture from multiple channels".

### Fichier modifiÃ©
- `/app/app/services/pm_orchestrator_service_v2.py` - lignes 1185-1195

### Impact
Ce bug explique pourquoi tous les tests via le workflow complet (PM Orchestrator) produisaient des UCs incohÃ©rentes alors que les tests individuels fonctionnaient correctement.


---

## 10. Sophie (PM) - Descriptions BR trop gÃ©nÃ©riques

**ObservÃ© lors de:** Test Olivia #102 avec BR-004

### ProblÃ¨me

Sophie extrait des BRs avec des descriptions trop courtes/gÃ©nÃ©riques :

**Exemple BR-004 :**
```json
{
  "id": "BR-004",
  "title": "Customer Information Association",
  "description": "Each service request must be linked to the customer who submitted it, including contact information."
}
```

**Ce qui manque :**
- Champs spÃ©cifiques (nom, email, tÃ©lÃ©phone, company ?)
- Type de relation Salesforce (Lookup Contact, Account ?)
- Gestion des clients inconnus (crÃ©ation auto ou erreur ?)
- Champs obligatoires vs optionnels
- RÃ¨gles de validation

### Impact

Olivia doit "inventer" les dÃ©tails manquants pour gÃ©nÃ©rer des UCs complets, ce qui peut crÃ©er :
- IncohÃ©rences entre agents
- Hallucinations sur les rÃ¨gles mÃ©tier
- Divergences avec les attentes client

### Solution proposÃ©e

Modifier le prompt de Sophie pour exiger :
1. **3-5 phrases minimum** par description de BR
2. **Champs/donnÃ©es spÃ©cifiques** mentionnÃ©s
3. **RÃ¨gles mÃ©tier** explicites
4. **DÃ©pendances** avec autres BRs identifiÃ©es

### Statut
[ ] Ã€ corriger dans `salesforce_pm.py`



---

## 11. ğŸš¨ TRONCATURE GÃ‰NÃ‰RALISÃ‰E - Limite tokens atteinte sur plusieurs agents

**Date:** 07 DÃ©cembre 2025

### Agents affectÃ©s

| Agent | ModÃ¨le | SymptÃ´me | Exec |
|-------|--------|----------|------|
| Olivia (BA) | Sonnet | UC-004-05 tronquÃ©e (5Ã¨me UC incomplÃ¨te) | #102 |
| Elena (QA) | Haiku | Document tronquÃ© Ã  TEST-033 sur 280+ prÃ©vus | #95 |
| Jordan (DevOps) | Haiku | Sections 5-10 non gÃ©nÃ©rÃ©es | #97 |

### Cause
Les agents gÃ©nÃ¨rent des documents trÃ¨s longs (50-100+ pages) qui dÃ©passent les limites de tokens :
- Haiku : ~8K output tokens â†’ ~25-30K chars max
- Sonnet : ~64K output tokens mais atteint aussi des limites sur contenus trÃ¨s longs

### Impact critique - Volume UCs

**Observation (test #102) :**
- 1 BR gÃ©nÃ¨re ~5 UCs
- Chaque UC = ~7K chars
- **1 BR = ~35K chars d'UCs**

**Projection pour workflow complet :**
- 27 BRs â†’ ~135 UCs
- Volume total : ~950K chars
- Risque de troncature systÃ©matique sur les derniers BRs

### Solutions Ã  Ã©valuer

1. **Upgrader les workers critiques vers Sonnet** (Elena, Jordan, Aisha, Lucas)
   - CoÃ»t plus Ã©levÃ© mais outputs complets
   
2. **DÃ©couper la gÃ©nÃ©ration en plusieurs appels**
   - GÃ©nÃ©rer section par section
   - Plus complexe Ã  implÃ©menter
   
3. **RÃ©duire la verbositÃ© des prompts**
   - Outputs plus concis mais moins dÃ©taillÃ©s
   - Risque de perte de qualitÃ©

4. **Limiter le nombre d'UCs par BR**
   - Max 3 UCs au lieu de 5
   - RÃ©duirait le volume total

### Statut
[ ] DÃ©cision Ã  prendre sur la stratÃ©gie


---

## 12. ğŸ“‹ RÃ‰CAPITULATIF - ProblÃ¨mes identifiÃ©s cette session

| # | ProblÃ¨me | PrioritÃ© | Statut |
|---|----------|----------|--------|
| 1 | Bug mapping BR workflow â†’ Olivia (query RAG vide) | ğŸ”´ CRITIQUE | âœ… CORRIGÃ‰ |
| 2 | Troncature Haiku (Elena, Jordan) | ğŸŸ  HAUTE | â³ Ã€ traiter |
| 3 | Troncature Sonnet (Olivia UC-004-05) | ğŸŸ  HAUTE | â³ Ã€ traiter |
| 4 | Volume UCs explosif (135 UCs / 950K chars pour 27 BRs) | ğŸŸ  HAUTE | â³ Ã€ traiter |
| 5 | Descriptions BR trop gÃ©nÃ©riques (Sophie) | ğŸŸ¡ MOYENNE | â³ Ã€ traiter |
| 6 | sentence_transformers manquant (reranker) | ğŸŸ¡ MOYENNE | â³ Ã€ traiter |
| 7 | Aisha appelÃ©e Ã  tort (greenfield vs migration) | ğŸŸ¡ MOYENNE | â³ Ã€ traiter |


---

## 13. ğŸ”´ PROBLÃˆMES DOCUMENT SDS - Ã€ traiter demain

**ObservÃ© lors des tests workflow complet**

### ProblÃ¨me 1 : Contenu dupliquÃ© sans valeur ajoutÃ©e

- **BRs apparaissent 2 fois** dans le document SDS
- Peu d'intÃ©rÃªt de rÃ©pÃ©ter les mÃªmes informations
- Gaspillage d'espace dans le document final

### ProblÃ¨me 2 : Contenu important tronquÃ© ou manquant

| Ã‰lÃ©ment | Statut actuel | Impact |
|---------|---------------|--------|
| Use Cases (UCs) | âŒ TronquÃ©s | Perte de dÃ©tails critiques |
| Tests (Elena) | âŒ Incomplets/manquants | Pas de plan de test complet |
| DÃ©ploiement (Jordan) | âŒ Manquant | Pas de stratÃ©gie DevOps |
| Data Migration (Aisha) | â“ Ã€ vÃ©rifier | Si pertinent selon projet |
| Formation (Lucas) | â“ Ã€ vÃ©rifier | Plan de formation |

### Actions pour demain

1. **Analyser la structure actuelle du template SDS**
   - Identifier les sections dupliquÃ©es
   - Identifier les sections manquantes

2. **Revoir la logique d'assemblage du document**
   - Comment les outputs agents sont intÃ©grÃ©s
   - Pourquoi certains sont tronquÃ©s

3. **Prioriser le contenu**
   - RÃ©duire/supprimer les duplications
   - S'assurer que les Ã©lÃ©ments critiques sont complets

### Fichiers Ã  examiner
- Template SDS : `/app/templates/sds_template.docx` (ou Ã©quivalent)
- Logique assemblage : chercher dans `pm_orchestrator_service_v2.py` ou service dÃ©diÃ©


---

## 14. ğŸ“‹ PLAN SESSION DEMAIN (08 DÃ©cembre 2025)

### Objectif 1 : Finaliser corrections agents et limites tokens

- [ ] DÃ©cider stratÃ©gie troncature (Sonnet pour tous ? DÃ©coupage ? RÃ©duction verbositÃ© ?)
- [ ] ImplÃ©menter la solution choisie
- [ ] Tester avec workflow complet

### Objectif 2 : Corriger le document SDS

- [ ] Analyser template et logique d'assemblage
- [ ] Supprimer duplications (BRs x2)
- [ ] S'assurer que UCs complets sont inclus
- [ ] Ajouter sections manquantes (Tests, DÃ©ploiement)
- [ ] Tester gÃ©nÃ©ration document complet

### Fichiers clÃ©s Ã  examiner

1. `pm_orchestrator_service_v2.py` - Logique workflow âœ… (corrigÃ© aujourd'hui)
2. Template/assemblage SDS - Ã€ examiner demain
3. Prompts agents (Elena, Jordan, Lucas) - VÃ©rifier outputs
4. `salesforce_pm.py` - AmÃ©liorer descriptions BR

